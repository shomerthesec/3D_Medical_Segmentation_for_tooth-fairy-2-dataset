{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ------------------------------------------------------------------\n# 1️⃣ Environment setup: install MONAI + other utilities\n# ------------------------------------------------------------------\n!pip install monai nibabel tqdm matplotlib SimpleITK itk\n\n# ------------------------------------------------------------------\n# 2️⃣ Imports\n# ------------------------------------------------------------------\nimport os, json, random, numpy as np, torch, nibabel as nib\nfrom pathlib import Path\nfrom glob import glob\nfrom tqdm.auto import tqdm\n\nimport monai\nfrom monai.transforms import (\n    Compose, LoadImaged , ScaleIntensityd,\n    RandFlipd, RandRotate90d, RandAffined,\n    CropForegroundd, ResizeWithPadOrCropd,EnsureChannelFirstd\n)\nfrom monai.networks.nets import UNet\nfrom monai.losses import DiceLoss, FocalLoss\nfrom monai.metrics import DiceMetric\nfrom monai.data import Dataset, DataLoader, CacheDataset\n\n# ------------------------------------------------------------------\n# 3️⃣ Seed everything for reproducibility\n# ------------------------------------------------------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\n# ------------------------------------------------------------------\n# 4️⃣ Device\n# ------------------------------------------------------------------\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:12:17.085817Z","iopub.execute_input":"2025-11-20T10:12:17.086349Z","iopub.status.idle":"2025-11-20T10:14:38.586113Z","shell.execute_reply.started":"2025-11-20T10:12:17.086320Z","shell.execute_reply":"2025-11-20T10:14:38.585281Z"}},"outputs":[{"name":"stdout","text":"Collecting monai\n  Downloading monai-1.5.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (5.3.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: SimpleITK in /usr/local/lib/python3.11/dist-packages (2.5.2)\nCollecting itk\n  Downloading itk-5.4.4.post1-cp311-abi3-manylinux_2_28_x86_64.whl.metadata (22 kB)\nRequirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.11/dist-packages (from monai) (1.26.4)\nRequirement already satisfied: torch>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from monai) (2.6.0+cu124)\nRequirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel) (6.5.2)\nRequirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel) (25.0)\nRequirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel) (4.15.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nCollecting itk-core==5.4.4.post1 (from itk)\n  Downloading itk_core-5.4.4.post1-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)\nCollecting itk-numerics==5.4.4.post1 (from itk)\n  Downloading itk_numerics-5.4.4.post1-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)\nCollecting itk-io==5.4.4.post1 (from itk)\n  Downloading itk_io-5.4.4.post1-cp311-abi3-manylinux_2_28_x86_64.whl.metadata (22 kB)\nCollecting itk-filtering==5.4.4.post1 (from itk)\n  Downloading itk_filtering-5.4.4.post1-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)\nCollecting itk-registration==5.4.4.post1 (from itk)\n  Downloading itk_registration-5.4.4.post1-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)\nCollecting itk-segmentation==5.4.4.post1 (from itk)\n  Downloading itk_segmentation-5.4.4.post1-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->monai) (3.20.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->monai) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->monai) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->monai) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.1->monai)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.1->monai)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.1->monai)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.1->monai)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.1->monai)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.1->monai)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.1->monai)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.1->monai)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.1->monai)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->monai) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->monai) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->monai) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.1->monai)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->monai) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->monai) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.1->monai) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.1->monai) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.24->monai) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.24->monai) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.24->monai) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.24->monai) (2024.2.0)\nDownloading monai-1.5.1-py3-none-any.whl (2.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading itk-5.4.4.post1-cp311-abi3-manylinux_2_28_x86_64.whl (16 kB)\nDownloading itk_core-5.4.4.post1-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (80.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading itk_filtering-5.4.4.post1-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (67.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading itk_io-5.4.4.post1-cp311-abi3-manylinux_2_28_x86_64.whl (28.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.0/28.0 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading itk_numerics-5.4.4.post1-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (57.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading itk_registration-5.4.4.post1-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (28.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.5/28.5 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading itk_segmentation-5.4.4.post1-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (15.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, itk-core, itk-numerics, itk-filtering, itk-segmentation, itk-registration, itk-io, monai, itk\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed itk-5.4.4.post1 itk-core-5.4.4.post1 itk-filtering-5.4.4.post1 itk-io-5.4.4.post1 itk-numerics-5.4.4.post1 itk-registration-5.4.4.post1 itk-segmentation-5.4.4.post1 monai-1.5.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"},{"name":"stderr","text":"<frozen importlib._bootstrap_external>:1241: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n2025-11-20 10:14:18.788336: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763633658.980520      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763633659.031342      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ------------------------------------------------------------------\n# 5️⃣ Dataset locations (Kaggle format)\n# ------------------------------------------------------------------\nBASE_DIR = Path(\"/kaggle/input/tooth-fairy-2-dataset/Dataset112_ToothFairy2\")\nIMG_DIR  = BASE_DIR / \"imagesTr\"\nLAB_DIR  = BASE_DIR / \"labelsTr\"\nJSON_PATH= BASE_DIR / \"dataset.json\"\n\nprint(f\"Images dir: {IMG_DIR}\")\nprint(f\"Labels dir: {LAB_DIR}\")\nprint(f\"JSON file : {JSON_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:14:38.587651Z","iopub.execute_input":"2025-11-20T10:14:38.588290Z","iopub.status.idle":"2025-11-20T10:14:38.593750Z","shell.execute_reply.started":"2025-11-20T10:14:38.588265Z","shell.execute_reply":"2025-11-20T10:14:38.593029Z"}},"outputs":[{"name":"stdout","text":"Images dir: /kaggle/input/tooth-fairy-2-dataset/Dataset112_ToothFairy2/imagesTr\nLabels dir: /kaggle/input/tooth-fairy-2-dataset/Dataset112_ToothFairy2/labelsTr\nJSON file : /kaggle/input/tooth-fairy-2-dataset/Dataset112_ToothFairy2/dataset.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ------------------------------------------------------------------\n# 7️⃣ Build lists of file paths expected by MONAI\n# ------------------------------------------------------------------\ndef build_file_dicts():\n    images = sorted(glob(str(IMG_DIR / \"*.mha\")))\n    labels = sorted(glob(str(LAB_DIR / \"*.mha\")))\n    \n    # Ensure same ordering\n    assert len(images) == len(labels), \"Image/label count mismatch!\"\n    \n    file_dicts = []\n    for img_path, lbl_path in zip(images, labels):\n        file_dicts.append({\"image\": img_path, \"label\": lbl_path})\n    return file_dicts\n\nfile_dicts = build_file_dicts()\nprint(f\"Prepared {len(file_dicts)} image/label pairs.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:14:38.594646Z","iopub.execute_input":"2025-11-20T10:14:38.594877Z","iopub.status.idle":"2025-11-20T10:14:38.698317Z","shell.execute_reply.started":"2025-11-20T10:14:38.594860Z","shell.execute_reply":"2025-11-20T10:14:38.697665Z"}},"outputs":[{"name":"stdout","text":"Prepared 480 image/label pairs.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ------------------------------------------------------------------\n# 8️⃣ Train/val split (80/20)\n# ------------------------------------------------------------------\nfrom sklearn.model_selection import train_test_split\n\ntrain_dicts, val_dicts = train_test_split(\n    file_dicts,\n    test_size=0.2,\n    random_state=SEED,\n    shuffle=True\n)\n\nprint(f\"Train size: {len(train_dicts)}, Val size: {len(val_dicts)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:14:38.699052Z","iopub.execute_input":"2025-11-20T10:14:38.699284Z","iopub.status.idle":"2025-11-20T10:14:38.716888Z","shell.execute_reply.started":"2025-11-20T10:14:38.699259Z","shell.execute_reply":"2025-11-20T10:14:38.716320Z"}},"outputs":[{"name":"stdout","text":"Train size: 384, Val size: 96\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ------------------------------------------------------------------\n# 9️⃣ Data augmentation & preprocessing (training)\n# ------------------------------------------------------------------\ntrain_transforms = Compose([\n    LoadImaged(keys=[\"image\", \"label\"], reader=\"ITKReader\"),\n    EnsureChannelFirstd(keys=[\"image\", \"label\"]),          # (C, D, H, W)\n    ScaleIntensityd(keys=[\"image\"]),\n    RandFlipd(keys=[\"image\", \"label\"], spatial_axis=0, prob=0.5),\n    RandFlipd(keys=[\"image\", \"label\"], spatial_axis=1, prob=0.5),\n    RandFlipd(keys=[\"image\", \"label\"], spatial_axis=2, prob=0.5),\n    RandRotate90d(keys=[\"image\", \"label\"], max_k=3, prob=0.5),\n    RandAffined(\n        keys=[\"image\", \"label\"],\n        rotate_range=(0, 0, np.pi/12),   # ±15°\n        scale_range=(0.9, 1.1),\n        mode=(\"bilinear\", \"nearest\"),\n        prob=0.5,\n    ),\n    ResizeWithPadOrCropd(\n        keys=[\"image\", \"label\"],\n        spatial_size=(128, 128, 128),   # adjust to your GPU memory\n    ),\n])\n\n# ------------------------------------------------------------------\n# Validation (no augmentation)\n# ------------------------------------------------------------------\nval_transforms = Compose([\n    LoadImaged(keys=[\"image\", \"label\"], reader=\"ITKReader\"),\n    EnsureChannelFirstd(keys=[\"image\", \"label\"]),          # (C, D, H, W)\n    ScaleIntensityd(keys=[\"image\"]),\n    ResizeWithPadOrCropd(\n        keys=[\"image\", \"label\"],\n        spatial_size=(128, 128, 128),\n    ),\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:14:38.718733Z","iopub.execute_input":"2025-11-20T10:14:38.718911Z","iopub.status.idle":"2025-11-20T10:14:38.730254Z","shell.execute_reply.started":"2025-11-20T10:14:38.718897Z","shell.execute_reply":"2025-11-20T10:14:38.729659Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\ntrain_ds = Dataset(\n    data=train_dicts,\n    transform=train_transforms,\n)\n\nval_ds = Dataset(\n    data=val_dicts,\n    transform=val_transforms,\n)\n\n# ------------------------------------------------------------------\n# 11️⃣ DataLoaders\n# ------------------------------------------------------------------\nBATCH_SIZE = 2\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    pin_memory=True,\n)\n\nval_loader = DataLoader(\n    val_ds,\n    batch_size=1,            # inference‑style\n    shuffle=False,\n    pin_memory=True,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:14:38.730878Z","iopub.execute_input":"2025-11-20T10:14:38.731118Z","iopub.status.idle":"2025-11-20T10:14:38.751484Z","shell.execute_reply.started":"2025-11-20T10:14:38.731091Z","shell.execute_reply":"2025-11-20T10:14:38.750777Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ------------------------------------------------------------------\n# 12️⃣ Model: 3‑D UNet (monai implementation)\n# ------------------------------------------------------------------\nmodel = UNet(\n    spatial_dims=3,\n    in_channels=1,          # mono‑channel CBCT\n    out_channels=49,        # labels\n    channels=(16, 32, 64, 128),\n    strides=(2, 2, 2),\n    num_res_units=1,\n).to(device)\n\n# ------------------------------------------------------------------\n# 13️⃣ Losses (Dice + Focal)\n# ------------------------------------------------------------------\ndice_loss = DiceLoss(to_onehot_y=True, softmax=True, squared_pred=False)\nfocal_loss = FocalLoss(to_onehot_y=True, use_softmax=True)\n\ndef loss_fn(pred, target):\n    return dice_loss(pred, target) + focal_loss(pred, target)\n\n# ------------------------------------------------------------------\n# 14️⃣ Optimizer & scheduler\n# ------------------------------------------------------------------\nLEARNING_RATE = 1e-3\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, T_max=20, eta_min=1e-5\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:14:38.752183Z","iopub.execute_input":"2025-11-20T10:14:38.752349Z","iopub.status.idle":"2025-11-20T10:14:38.917758Z","shell.execute_reply.started":"2025-11-20T10:14:38.752336Z","shell.execute_reply":"2025-11-20T10:14:38.917190Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ------------------------------------------------------------------\n# 15️⃣ Helper to calculate Dice per class on a batch\n# ------------------------------------------------------------------\ndef dice_per_class(pred, target):\n    # pred: (B, C, D, H, W), target: (B, D, H, W)\n    dice_metric = DiceMetric(include_background=True, reduction=\"none\")\n    with torch.no_grad():\n        d_metric= dice_metric(pred.softmax(dim=1), target).cpu().numpy()\n        return np.where(np.isnan(d_metric), 0, d_metric)\n\n\n\n# ------------------------------------------------------------------\n# 16️⃣ Training\n# ------------------------------------------------------------------\nNUM_EPOCHS = 5   # set higher for real training\nBEST_VAL_DICE= -1.0\nfor epoch in tqdm(range(NUM_EPOCHS)):\n    model.train()\n    train_loss = 0.0\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n        images = batch[\"image\"].to(device, dtype=torch.float32)\n        labels = batch[\"label\"].to(device, dtype=torch.long)  # shape (B, D, H, W)\n\n        optimizer.zero_grad()\n        outputs = model(images)          # (B, C, D, H, W)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * images.size(0)\n        # print(\"dice per class= \",dice_per_class(outputs, labels) )\n\n    train_loss /= len(train_loader.dataset)\n    scheduler.step()\n\n    # Validation Dice\n    model.eval()\n    dice_vals = []\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Val\"):\n            images = batch[\"image\"].to(device)\n            labels = batch[\"label\"].to(device)\n            outputs = model(images)\n\n            # compute Dice per class\n            dice_vals.append(dice_per_class(outputs, labels))\n\n    mean_dice = np.array(dice_vals).mean(axis=1)\n    print(f\"Epoch {epoch+1:02d} | Train loss: {train_loss:.4f} # | \"\n          f\"Val Dice (mean): {np.mean(mean_dice):.4f}\")  # ignore background\n\n    current_val_dice= np.mean(mean_dice)\n    if current_val_dice > BEST_VAL_DICE:\n            BEST_VAL_DICE = current_val_dice\n            CHECKPOINT_DIR= \"checkpoints\"\n            # Create directory if it doesn't exist\n            if not os.path.exists(CHECKPOINT_DIR):\n                os.makedirs(CHECKPOINT_DIR)\n                \n            checkpoint_path = os.path.join(CHECKPOINT_DIR, \"best_model_checkpoint.pt\")\n            \n            # Save the full state\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'best_val_dice': BEST_VAL_DICE,\n                'train_loss': train_loss,\n            }, checkpoint_path)\n            \n            print(f\"--> Checkpoint saved to {checkpoint_path}! New best Val Dice: {BEST_VAL_DICE:.4f}\")","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-11-20T10:14:38.918462Z","iopub.execute_input":"2025-11-20T10:14:38.918674Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4859d2f8b8fd4341b44d84eaa67386a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1/5:   0%|          | 0/192 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e259f7b500242b8b35ee1ef2fe438f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/96 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ac62f90d36b476299bf9e77071d427b"}},"metadata":{}},{"name":"stdout","text":"Epoch 01 | Train loss: 0.9726 # | Val Dice (mean): 0.0214\n--> Checkpoint saved to checkpoints/best_model_checkpoint.pt! New best Val Dice: 0.0214\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/5:   0%|          | 0/192 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0d6716583af48f0b573bc4ca3ca48fa"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"mean_dice = np.array(dice_vals).mean(axis=1)\nprint(mean_dice.shape)\nnp.mean(mean_dice,axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------------\n# 17️⃣ Sliding window inference helper (monai provides this)\n# ------------------------------------------------------------------\nfrom monai.inferers import sliding_window_inference\n\ndef infer_volume(img_path, model, device=\"cuda\", roi_size=(128,128,128),\n                 sw_batch_size=2, overlap=0.5, tta=False):\n    \"\"\"\n    Args:\n        img_path (str): Path to a single .nii.gz/.mha volume\n        model   : Trained MONAI model\n        roi_size: Size of the sliding window\n        sw_batch_size: Batch size for sliding inference\n    Returns:\n        pred_mask (np.ndarray): 3‑D array of class indices (0–32)\n    \"\"\"\n    # Load & preprocess\n    img = nib.load(img_path).get_fdata()\n    # rescale to [0,1]\n    img = (img - np.min(img)) / (np.max(img) - np.min(img))\n    img = torch.from_numpy(img).unsqueeze(0).unsqueeze(0).float()  # (1,1,D,H,W)\n    \n    if tta:\n        # Simple TTA: horizontal + vertical flip\n        flips = [lambda x: x,  # identity\n                 lambda x: torch.flip(x, dims=[3]),  # flip H\n                 lambda x: torch.flip(x, dims=[4])]  # flip W\n        preds = []\n        for f in flips:\n            inp = f(img.to(device))\n            out = sliding_window_inference(inp, roi_size, sw_batch_size,\n                                           model)\n            preds.append(out.cpu())\n        out = torch.mean(torch.stack(preds), dim=0)  # average predictions\n    else:\n        out = sliding_window_inference(img.to(device), roi_size, sw_batch_size,\n                                       model)\n\n    # Convert logits to class indices\n    pred_mask = torch.argmax(out, dim=1).squeeze(0).cpu().numpy()  # (D,H,W)\n    return pred_mask\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------------\n# 18️⃣ Map class indices to FDI numbers (1–32)\n# ------------------------------------------------------------------\nFDI_MAP = {i: f\"{i}\" for i in range(1, 33)}   # simple mapping; adjust if needed\n\ndef save_mask(mask, out_path, reference_img):\n    \"\"\"\n    mask: (D,H,W) int array with values 0–32\n    reference_img: nib.Nifti1Image to copy affine & header\n    \"\"\"\n    # Convert mask to NIfTI\n    nifti = nib.Nifti1Image(mask.astype(np.int16), affine=reference_img.affine,\n                            header=reference_img.header)\n    nib.save(nifti, out_path)\n\n# ------------------------------------------------------------------\n# 19️⃣ Demo inference on one volume\n# ------------------------------------------------------------------\nsample_img = val_dicts[0][\"image\"]\nsample_lbl = val_dicts[0][\"label\"]\n\npred_mask = infer_volume(sample_img, model, device=device,\n                         roi_size=(128,256,256),\n                         sw_batch_size=2,\n                         overlap=0.5,\n                         tta=True)\n\n# Load reference header for saving\nref_img = nib.load(sample_img)\nout_path = f\"/kaggle/working/pred_{Path(sample_img).stem}.nii.gz\"\nsave_mask(pred_mask, out_path, ref_img)\n\nprint(f\"Saved prediction to {out_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------------\n# 20️⃣ Compute Dice per tooth over the whole validation set\n# ------------------------------------------------------------------\nall_dice = []\nfor batch in tqdm(val_loader, desc=\"Eval Dice\"):\n    images = batch[\"image\"].to(device)\n    labels = batch[\"label\"].to(device)\n\n    with torch.no_grad():\n        outputs = model(images)\n        dice_batch = dice_per_class(outputs, labels)   # (B, C)\n    all_dice.append(dice_batch)\n\nall_dice = np.concatenate(all_dice, axis=0)   # (N_samples, C)\n\n# Average Dice per class\nmean_dice_per_class = np.mean(all_dice, axis=0)   # (C,)\nprint(\"Mean Dice per class:\")\nfor idx, d in enumerate(mean_dice_per_class):\n    print(f\"  Class {idx:02d}: {d:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------------\n# 21️⃣ Save the best model (simple example)\n# ------------------------------------------------------------------\ntorch.save(model.state_dict(), \"/kaggle/working/best_unet3d.pth\")\nprint(\"Model checkpoint saved.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}